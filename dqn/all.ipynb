{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from gym import spaces\n",
    "from torch.optim import Optimizer\n",
    "import numpy as np\n",
    "# from dqn.model import DQN\n",
    "# from dqn.replay_buffer import ReplayBuffer\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "import random\n",
    "from nle import nethack\n",
    "import minihack\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    \"\"\"Formats state into form that the NN can accept\"\"\"\n",
    "    glyphs = state[\"glyphs\"]\n",
    "    # Normalize\n",
    "    glyphs = glyphs/glyphs.max()\n",
    "    glyphs = glyphs.reshape((1,1,21,79))\n",
    "    return torch.from_numpy(glyphs).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_screen(state):\n",
    "    \"\"\"Displays the state as the screen in image form using the 'pixel' observation key\"\"\"\n",
    "    screen = Image.fromarray(np.uint8(state['pixel']))\n",
    "    display(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Simple storage for transitions from an environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Initialise a buffer of a given size for storing transitions\n",
    "        :param size: the maximum number of transitions that can be stored\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
    "        :param state: the agent's initial state\n",
    "        :param action: the action taken by the agent\n",
    "        :param reward: the reward the agent received\n",
    "        :param next_state: the subsequent state\n",
    "        :param done: whether the episode terminated\n",
    "        \"\"\"\n",
    "        data = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, indices):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in indices:\n",
    "            data = self._storage[i]\n",
    "            state, action, reward, next_state, done = data\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the buffer.\n",
    "        :param batch_size: the number of transitions to sample\n",
    "        :return: a mini-batch of sampled transitions\n",
    "        \"\"\"\n",
    "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
    "        return self._encode_sample(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of a Deep Q-Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space: spaces.Discrete):\n",
    "        \"\"\"\n",
    "        Initialise the DQN\n",
    "        :param action_space: the action space of the environment\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20,kernel_size=(5, 5))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50,kernel_size=(5, 5))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = nn.Linear(in_features=1600, out_features=500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=action_space.n)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns the values of a forward pass of the network\n",
    "        :param x: The input to feed into the network \n",
    "        \"\"\"\n",
    "        # define first conv layer with max pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # define second conv layer with max pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # Define fully connected layers\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class that brings all the DQN compenents together so that a model can be trained\n",
    "class DQNAgent():\n",
    "    def __init__(self, observation_space, action_space, **kwargs):\n",
    "        global device\n",
    "        self.action_space = action_space\n",
    "        self.replay_buffer = kwargs.get(\"replay_buffer\", None)\n",
    "        self.use_double_dqn = kwargs.get(\"use_double_dqn\", None)\n",
    "        self.gamma = kwargs.get(\"gamma\", 0.99)\n",
    "        self.lr = kwargs.get(\"lr\", None)\n",
    "        self.betas = kwargs.get(\"betas\", (0.9, 0.999))\n",
    "        self.batch_size = kwargs.get(\"batch_size\", None)\n",
    "        # Create the online and target network\n",
    "        self.online_network = DQN(action_space).to(device)\n",
    "        self.target_network = DQN(action_space).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.online_network.parameters(), lr=self.lr, betas=self.betas)\n",
    "   \n",
    "\n",
    "    def optimise_td_loss(self):\n",
    "        \"\"\"\n",
    "        Optimise the TD-error over a single minibatch of transitions\n",
    "        :return: the loss\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.use_double_dqn:\n",
    "                _, max_next_action = self.online_network(next_states).max(1)\n",
    "                max_next_q_values = self.target_network(next_states).gather(1, max_next_action.unsqueeze(1)).squeeze()\n",
    "            else:\n",
    "                next_q_values = self.online_network(next_states)\n",
    "                max_next_q_values, _ = next_q_values.max(1)\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        input_q_values = self.target_network(states)\n",
    "        input_q_values = input_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = F.smooth_l1_loss(input_q_values, target_q_values)\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        del states\n",
    "        del next_states\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Update the target Q-network by copying the weights from the current Q-network\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\"Select action base on network inference\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            observation = observation.type(torch.FloatTensor) \n",
    "        else:\n",
    "            observation = observation.type(torch.cuda.FloatTensor) \n",
    "        state = torch.unsqueeze(observation, 0).to(device)\n",
    "        result = self.online_network.forward(state)\n",
    "        action = torch.argmax(result).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(env, seed, learning_rate, max_episodes, max_episode_length, gamma, verbose=True):\n",
    "\n",
    "    hyper_params = {\n",
    "        'replay-buffer-size': 100000, # replay buffer size\n",
    "        'gamma': 0.99,  # discount factor\n",
    "        'learning-rate': learning_rate, # learning rate for Adam optimizer\n",
    "        'num-steps': int(2e5),  # total number of steps to run the environment for\n",
    "        'batch-size': 256,  # number of transitions to optimize at the same time\n",
    "        'learning-starts': 50000,  # set learning to start after 1000 steps of exploration\n",
    "        'learning-freq': 5,  # number of iterations between every optimization step\n",
    "        'use-double-dqn': True, # use double deep Q-learning\n",
    "        'target-update-freq': 50000, # number of iterations between every target network update\n",
    "        'eps-start': 1.0,  # e-greedy start threshold \n",
    "        'eps-end': 0.01,  # e-greedy end threshold \n",
    "        'eps-fraction': 0.4,  # fraction of num-steps\n",
    "        'print-freq': 10,\n",
    "\n",
    "    }\n",
    "\n",
    "    np.random.seed(42)\n",
    "    env.seed(42)\n",
    "    \n",
    "    # Create DQN agent\n",
    "    replay_buffer = ReplayBuffer(hyper_params['replay-buffer-size'])\n",
    "    agent = DQNAgent(\n",
    "        env.observation_space, \n",
    "        env.action_space,\n",
    "        train=True,\n",
    "        replay_buffer=replay_buffer,\n",
    "        use_double_dqn=hyper_params['use-double-dqn'],\n",
    "        lr=hyper_params['learning-rate'],\n",
    "        batch_size=hyper_params['batch-size'],\n",
    "        gamma=hyper_params['gamma'],\n",
    "    )\n",
    "    \n",
    "    # define variables to track agent metrics\n",
    "    total_reward = 0\n",
    "    scores = []\n",
    "    mean_rewards = []\n",
    "\n",
    "    eps_timesteps = hyper_params[\"eps-fraction\"] * float(hyper_params[\"num-steps\"])\n",
    "    episode_rewards = [0.0]\n",
    "\n",
    "    # Reset gym env before training\n",
    "    state = format_state(env.reset())\n",
    "    eps_timesteps = hyper_params['eps-fraction'] * float(hyper_params['num-steps'])\n",
    "    # Train for set number of steps\n",
    "    for t in range(hyper_params['num-steps']):\n",
    "        # determine exploration probability\n",
    "        fract = min(1.0, float(t) / eps_timesteps)\n",
    "        eps_threshold = hyper_params[\"eps-start\"] + fract * (hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"])\n",
    "        sample = random.random()\n",
    "        # Decide to explore and choose random action or use model to act\n",
    "        if sample < eps_threshold:\n",
    "            action = np.random.choice(agent.action_space.n)\n",
    "        else:\n",
    "            action = agent.act(state)\n",
    "        # Take step in environment\n",
    "        (next_state, reward, done, _) = env.step(action)\n",
    "        next_state = format_state(next_state)\n",
    "        replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            scores.append(total_reward)\n",
    "            print(f\"episode reward: {total_reward}\")\n",
    "            np.random.seed(seed)\n",
    "            env.seed(seed)\n",
    "            state = format_state(env.reset())\n",
    "            total_reward = 0\n",
    "\n",
    "        if (\n",
    "            t > hyper_params[\"learning-starts\"]\n",
    "            and t % hyper_params[\"learning-freq\"] == 0\n",
    "        ):\n",
    "            agent.optimise_td_loss()\n",
    "\n",
    "        if (\n",
    "            t > hyper_params[\"learning-starts\"]\n",
    "            and t % hyper_params[\"target-update-freq\"] == 0\n",
    "        ):\n",
    "            agent.update_target_network()\n",
    "\n",
    "        num_episodes = len(scores)\n",
    "        if done and hyper_params['print-freq'] is not None and len(scores) % hyper_params['print-freq'] == 0:\n",
    "            mean_100ep_reward = round(np.mean(scores[-101:-1]), 1)\n",
    "            mean_rewards.append(mean_100ep_reward)\n",
    "            print('********************************************************')\n",
    "            print('steps: {}'.format(t))\n",
    "            print('episodes: {}'.format(num_episodes))\n",
    "            print('mean 100 episode reward: {}'.format(mean_100ep_reward))\n",
    "            print('% time spent exploring: {}'.format(eps_threshold))\n",
    "            print('********************************************************')\n",
    "  \n",
    "        if num_episodes >=max_episodes:\n",
    "            return scores\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(env,number_episodes,max_episode_length,iterations):\n",
    "    \"\"\"Trains DQN model for a number of episodes on a given environment\"\"\"\n",
    "    seeds = np.random.randint(1000, size=iterations)\n",
    "    scores_arr = [] \n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Train the DQN Model \n",
    "        scores = dqn(env=env, \n",
    "                        seed=42, \n",
    "                        learning_rate=1e-6,\n",
    "                        max_episodes=number_episodes, \n",
    "                        max_episode_length=max_episode_length, \n",
    "                        gamma=0.99 ,\n",
    "                        verbose=True)\n",
    "        # Store rewards for this iteration \n",
    "        scores_arr.append(scores)\n",
    "        \n",
    "    return scores_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1264x336>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MOVE_ACTIONS = tuple(nethack.CompassDirection)\n",
    "NAVIGATE_ACTIONS = MOVE_ACTIONS + (\n",
    "    nethack.Command.PICKUP,\n",
    "    nethack.Command.APPLY,\n",
    "    nethack.Command.FIRE,\n",
    "    nethack.Command.RUSH,\n",
    "    nethack.Command.ZAP,\n",
    "    nethack.Command.PUTON,\n",
    "    nethack.Command.READ,\n",
    "    nethack.Command.WEAR,\n",
    "    nethack.Command.QUAFF,\n",
    "    nethack.Command.PRAY,\n",
    "    )\n",
    "env = gym.make(\"MiniHack-Quest-Hard-v0\", observation_keys=[\"glyphs\",\"pixel\",\"message\"], actions=NAVIGATE_ACTIONS)\n",
    "# Reset the environment and display the screen of the starting state \n",
    "display_screen(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward: -1.410000000000001\n",
      "episode reward: -1.8400000000000014\n",
      "episode reward: -1.480000000000001\n",
      "episode reward: -1.420000000000001\n",
      "episode reward: -2.2799999999999954\n",
      "episode reward: -1.8400000000000014\n",
      "episode reward: -1.8700000000000014\n",
      "episode reward: -1.8000000000000014\n",
      "episode reward: -1.5400000000000011\n",
      "episode reward: -1.8800000000000014\n",
      "********************************************************\n",
      "steps: 2252\n",
      "episodes: 10\n",
      "mean 100 episode reward: -1.7\n",
      "% time spent exploring: 0.9721315\n",
      "********************************************************\n",
      "episode reward: -1.9200000000000015\n",
      "episode reward: -1.8200000000000014\n",
      "episode reward: -2.209999999999997\n",
      "episode reward: -0.6300000000000003\n",
      "episode reward: -1.480000000000001\n",
      "episode reward: -1.5300000000000011\n",
      "episode reward: -0.46000000000000024\n",
      "episode reward: -2.3299999999999943\n",
      "episode reward: -0.5900000000000003\n",
      "episode reward: -1.5300000000000011\n",
      "********************************************************\n",
      "steps: 4132\n",
      "episodes: 20\n",
      "mean 100 episode reward: -1.6\n",
      "% time spent exploring: 0.9488665000000001\n",
      "********************************************************\n",
      "episode reward: -0.35000000000000014\n",
      "episode reward: -1.5400000000000011\n",
      "episode reward: -0.34000000000000014\n",
      "episode reward: -1.5200000000000011\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.35000000000000014\n",
      "episode reward: -0.5700000000000003\n",
      "episode reward: -1.9400000000000015\n",
      "episode reward: -2.3599999999999937\n",
      "episode reward: -2.109999999999999\n",
      "********************************************************\n",
      "steps: 5679\n",
      "episodes: 30\n",
      "mean 100 episode reward: -1.4\n",
      "% time spent exploring: 0.929722375\n",
      "********************************************************\n",
      "episode reward: -0.7400000000000004\n",
      "episode reward: -0.6500000000000004\n",
      "episode reward: -1.8000000000000014\n",
      "episode reward: -0.3200000000000001\n",
      "episode reward: -0.49000000000000027\n",
      "episode reward: -1.6400000000000012\n",
      "episode reward: -1.7900000000000014\n",
      "episode reward: -0.3300000000000001\n",
      "episode reward: -0.6700000000000004\n",
      "episode reward: -1.5900000000000012\n",
      "********************************************************\n",
      "steps: 6966\n",
      "episodes: 40\n",
      "mean 100 episode reward: -1.3\n",
      "% time spent exploring: 0.91379575\n",
      "********************************************************\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.6700000000000004\n",
      "episode reward: -1.6500000000000012\n",
      "episode reward: -0.6700000000000004\n",
      "episode reward: -1.480000000000001\n",
      "episode reward: -0.6300000000000003\n",
      "episode reward: -1.5600000000000012\n",
      "episode reward: -0.5000000000000002\n",
      "episode reward: -0.35000000000000014\n",
      "episode reward: -0.3300000000000001\n",
      "********************************************************\n",
      "steps: 8086\n",
      "episodes: 50\n",
      "mean 100 episode reward: -1.3\n",
      "% time spent exploring: 0.89993575\n",
      "********************************************************\n",
      "episode reward: -0.6200000000000003\n",
      "episode reward: -0.5600000000000003\n",
      "episode reward: -0.6300000000000003\n",
      "episode reward: -0.36000000000000015\n",
      "episode reward: -0.6100000000000003\n",
      "episode reward: -2.4199999999999924\n",
      "episode reward: -0.45000000000000023\n",
      "episode reward: -0.6400000000000003\n",
      "episode reward: -0.6200000000000003\n",
      "episode reward: -0.6600000000000004\n",
      "********************************************************\n",
      "steps: 9085\n",
      "episodes: 60\n",
      "mean 100 episode reward: -1.2\n",
      "% time spent exploring: 0.887573125\n",
      "********************************************************\n",
      "episode reward: -1.9700000000000015\n",
      "episode reward: -0.48000000000000026\n",
      "episode reward: -1.6500000000000012\n",
      "episode reward: -0.6700000000000004\n",
      "episode reward: -0.35000000000000014\n",
      "episode reward: -0.37000000000000016\n",
      "episode reward: -0.5000000000000002\n",
      "episode reward: -0.38000000000000017\n",
      "episode reward: -0.4300000000000002\n",
      "episode reward: -0.8400000000000005\n",
      "********************************************************\n",
      "steps: 10100\n",
      "episodes: 70\n",
      "mean 100 episode reward: -1.1\n",
      "% time spent exploring: 0.8750125\n",
      "********************************************************\n",
      "episode reward: -0.35000000000000014\n",
      "episode reward: -0.4000000000000002\n",
      "episode reward: -0.6500000000000004\n",
      "episode reward: -0.6200000000000003\n",
      "episode reward: -0.7600000000000005\n",
      "episode reward: -0.47000000000000025\n",
      "episode reward: -0.35000000000000014\n",
      "episode reward: -0.6400000000000003\n",
      "episode reward: -0.37000000000000016\n",
      "episode reward: -0.7200000000000004\n",
      "********************************************************\n",
      "steps: 10804\n",
      "episodes: 80\n",
      "mean 100 episode reward: -1.0\n",
      "% time spent exploring: 0.8663005\n",
      "********************************************************\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.6900000000000004\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -0.6800000000000004\n",
      "episode reward: -0.7300000000000004\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -0.6400000000000003\n",
      "episode reward: -0.6000000000000003\n",
      "episode reward: -0.49000000000000027\n",
      "********************************************************\n",
      "steps: 11650\n",
      "episodes: 90\n",
      "mean 100 episode reward: -1.0\n",
      "% time spent exploring: 0.85583125\n",
      "********************************************************\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -1.6300000000000012\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -0.6400000000000003\n",
      "episode reward: -2.159999999999998\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -2.109999999999999\n",
      "episode reward: -0.5400000000000003\n",
      "episode reward: -2.579999999999989\n",
      "episode reward: -1.7200000000000013\n",
      "********************************************************\n",
      "steps: 13391\n",
      "episodes: 100\n",
      "mean 100 episode reward: -1.0\n",
      "% time spent exploring: 0.834286375\n",
      "********************************************************\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -0.36000000000000015\n",
      "episode reward: -2.4999999999999907\n",
      "episode reward: -0.6900000000000004\n",
      "episode reward: -0.7500000000000004\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.7400000000000004\n",
      "episode reward: -0.35000000000000014\n",
      "episode reward: -0.6600000000000004\n",
      "********************************************************\n",
      "steps: 14449\n",
      "episodes: 110\n",
      "mean 100 episode reward: -0.9\n",
      "% time spent exploring: 0.821193625\n",
      "********************************************************\n",
      "episode reward: -0.6500000000000004\n",
      "episode reward: -0.7500000000000004\n",
      "episode reward: -0.36000000000000015\n",
      "episode reward: -0.7500000000000004\n",
      "episode reward: -0.7400000000000004\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -2.1699999999999977\n",
      "episode reward: -0.47000000000000025\n",
      "episode reward: -1.7900000000000014\n",
      "episode reward: -0.7100000000000004\n",
      "********************************************************\n",
      "steps: 15638\n",
      "episodes: 120\n",
      "mean 100 episode reward: -0.9\n",
      "% time spent exploring: 0.80647975\n",
      "********************************************************\n",
      "episode reward: -0.5400000000000003\n",
      "episode reward: -0.6400000000000003\n",
      "episode reward: -0.8200000000000005\n",
      "episode reward: -1.8300000000000014\n",
      "episode reward: -0.5100000000000002\n",
      "episode reward: -0.7600000000000005\n",
      "episode reward: -0.4300000000000002\n",
      "episode reward: -0.5800000000000003\n",
      "episode reward: -2.6499999999999875\n",
      "episode reward: -0.5200000000000002\n",
      "********************************************************\n",
      "steps: 16868\n",
      "episodes: 130\n",
      "mean 100 episode reward: -0.9\n",
      "% time spent exploring: 0.7912585\n",
      "********************************************************\n",
      "episode reward: -0.7500000000000004\n",
      "episode reward: -0.5800000000000003\n",
      "episode reward: -0.8100000000000005\n",
      "episode reward: -0.8200000000000005\n",
      "episode reward: -0.7900000000000005\n",
      "episode reward: -0.4400000000000002\n",
      "episode reward: -0.3900000000000002\n",
      "episode reward: -0.5200000000000002\n",
      "episode reward: -0.7100000000000004\n",
      "episode reward: -0.3900000000000002\n",
      "********************************************************\n",
      "steps: 17663\n",
      "episodes: 140\n",
      "mean 100 episode reward: -0.8\n",
      "% time spent exploring: 0.781420375\n",
      "********************************************************\n",
      "episode reward: -0.6900000000000004\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -2.339999999999994\n",
      "episode reward: -0.7800000000000005\n",
      "episode reward: -0.7700000000000005\n",
      "episode reward: -0.8700000000000006\n",
      "episode reward: -0.4300000000000002\n",
      "episode reward: -0.7600000000000005\n",
      "episode reward: -0.8400000000000005\n",
      "episode reward: -2.6899999999999866\n",
      "********************************************************\n",
      "steps: 19052\n",
      "episodes: 150\n",
      "mean 100 episode reward: -0.8\n",
      "% time spent exploring: 0.7642315\n",
      "********************************************************\n",
      "episode reward: -0.8300000000000005\n",
      "episode reward: -0.35000000000000014\n",
      "episode reward: -0.8000000000000005\n",
      "episode reward: -2.4999999999999907\n",
      "episode reward: -0.7600000000000005\n",
      "episode reward: -0.5000000000000002\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.6700000000000004\n",
      "episode reward: -0.6100000000000003\n",
      "episode reward: -2.8299999999999836\n",
      "********************************************************\n",
      "steps: 20399\n",
      "episodes: 160\n",
      "mean 100 episode reward: -0.8\n",
      "% time spent exploring: 0.747562375\n",
      "********************************************************\n",
      "episode reward: -0.8500000000000005\n",
      "episode reward: -0.8300000000000005\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.8500000000000005\n",
      "episode reward: -0.48000000000000026\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.5000000000000002\n",
      "episode reward: -0.6600000000000004\n",
      "episode reward: -0.5500000000000003\n",
      "episode reward: -0.6900000000000004\n",
      "********************************************************\n",
      "steps: 21261\n",
      "episodes: 170\n",
      "mean 100 episode reward: -0.9\n",
      "% time spent exploring: 0.736895125\n",
      "********************************************************\n",
      "episode reward: -2.9799999999999804\n",
      "episode reward: -0.4000000000000002\n",
      "episode reward: -2.959999999999981\n",
      "episode reward: -1.8800000000000014\n",
      "episode reward: -0.6200000000000003\n",
      "episode reward: -0.47000000000000025\n",
      "episode reward: -0.8400000000000005\n",
      "episode reward: -0.8500000000000005\n",
      "episode reward: -0.6400000000000003\n",
      "episode reward: -0.4100000000000002\n",
      "********************************************************\n",
      "steps: 22802\n",
      "episodes: 180\n",
      "mean 100 episode reward: -0.9\n",
      "% time spent exploring: 0.71782525\n",
      "********************************************************\n",
      "episode reward: -0.5800000000000003\n",
      "episode reward: -2.2799999999999954\n",
      "episode reward: -0.5600000000000003\n",
      "episode reward: -0.4300000000000002\n",
      "episode reward: -0.8400000000000005\n",
      "episode reward: -2.0400000000000005\n",
      "episode reward: -2.0400000000000005\n",
      "episode reward: -0.8800000000000006\n",
      "episode reward: -0.8000000000000005\n",
      "episode reward: -0.7000000000000004\n",
      "********************************************************\n",
      "steps: 24230\n",
      "episodes: 190\n",
      "mean 100 episode reward: -1.0\n",
      "% time spent exploring: 0.70015375\n",
      "********************************************************\n",
      "episode reward: -2.0400000000000005\n",
      "episode reward: -0.5800000000000003\n",
      "episode reward: -0.47000000000000025\n",
      "episode reward: -0.8500000000000005\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -0.8200000000000005\n",
      "episode reward: -0.9300000000000006\n",
      "episode reward: -0.45000000000000023\n",
      "episode reward: -0.9900000000000007\n",
      "episode reward: -0.6300000000000003\n",
      "********************************************************\n",
      "steps: 25311\n",
      "episodes: 200\n",
      "mean 100 episode reward: -0.9\n",
      "% time spent exploring: 0.686776375\n",
      "********************************************************\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -0.6300000000000003\n",
      "episode reward: -2.8299999999999836\n",
      "episode reward: -1.1100000000000008\n",
      "episode reward: -0.6500000000000004\n",
      "episode reward: -2.5199999999999902\n",
      "episode reward: -0.8900000000000006\n",
      "episode reward: -0.5100000000000002\n",
      "episode reward: -0.48000000000000026\n",
      "episode reward: -0.5000000000000002\n",
      "********************************************************\n",
      "steps: 26690\n",
      "episodes: 210\n",
      "mean 100 episode reward: -1.0\n",
      "% time spent exploring: 0.66971125\n",
      "********************************************************\n",
      "episode reward: -0.49000000000000027\n",
      "episode reward: -0.6200000000000003\n",
      "episode reward: -0.6900000000000004\n",
      "episode reward: -3.0799999999999783\n",
      "episode reward: -0.5900000000000003\n",
      "episode reward: -2.3099999999999947\n",
      "episode reward: -2.99999999999998\n",
      "episode reward: -0.7300000000000004\n",
      "episode reward: -0.49000000000000027\n",
      "episode reward: -0.7800000000000005\n",
      "********************************************************\n",
      "steps: 28285\n",
      "episodes: 220\n",
      "mean 100 episode reward: -1.0\n",
      "% time spent exploring: 0.649973125\n",
      "********************************************************\n",
      "episode reward: -3.1699999999999764\n",
      "episode reward: -0.9200000000000006\n",
      "episode reward: -0.7600000000000005\n",
      "episode reward: -0.47000000000000025\n",
      "episode reward: -3.229999999999975\n",
      "episode reward: -0.9100000000000006\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -0.6400000000000003\n",
      "episode reward: -0.5000000000000002\n",
      "episode reward: -2.6899999999999866\n",
      "********************************************************\n",
      "steps: 30051\n",
      "episodes: 230\n",
      "mean 100 episode reward: -1.0\n",
      "% time spent exploring: 0.628118875\n",
      "********************************************************\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -0.5200000000000002\n",
      "episode reward: -0.6900000000000004\n",
      "episode reward: -0.6800000000000004\n",
      "episode reward: -3.7299999999999645\n",
      "episode reward: -0.7600000000000005\n",
      "episode reward: -0.9900000000000007\n",
      "episode reward: -0.5000000000000002\n",
      "episode reward: -2.489999999999991\n",
      "episode reward: -0.6900000000000004\n",
      "********************************************************\n",
      "steps: 31515\n",
      "episodes: 240\n",
      "mean 100 episode reward: -1.1\n",
      "% time spent exploring: 0.610001875\n",
      "********************************************************\n",
      "episode reward: -1.0800000000000007\n",
      "episode reward: -0.6000000000000003\n",
      "episode reward: -0.5600000000000003\n",
      "episode reward: -0.7700000000000005\n",
      "episode reward: -0.6400000000000003\n",
      "episode reward: -1.0000000000000007\n",
      "episode reward: -3.559999999999968\n",
      "episode reward: -0.6300000000000003\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -2.489999999999991\n",
      "********************************************************\n",
      "steps: 33012\n",
      "episodes: 250\n",
      "mean 100 episode reward: -1.1\n",
      "% time spent exploring: 0.5914765\n",
      "********************************************************\n",
      "episode reward: -0.7800000000000005\n",
      "episode reward: -0.8200000000000005\n",
      "episode reward: -0.6100000000000003\n",
      "episode reward: -1.0400000000000007\n",
      "episode reward: -0.5300000000000002\n",
      "episode reward: -1.1400000000000008\n",
      "episode reward: -0.7500000000000004\n",
      "episode reward: -2.719999999999986\n",
      "episode reward: -1.1000000000000008\n",
      "episode reward: -4.019999999999959\n",
      "********************************************************\n",
      "steps: 34640\n",
      "episodes: 260\n",
      "mean 100 episode reward: -1.1\n",
      "% time spent exploring: 0.57133\n",
      "********************************************************\n",
      "episode reward: -0.7400000000000004\n",
      "episode reward: -1.430000000000001\n",
      "episode reward: -0.6700000000000004\n",
      "episode reward: -0.9800000000000006\n",
      "episode reward: -3.429999999999971\n",
      "episode reward: -0.7500000000000004\n",
      "episode reward: -0.5900000000000003\n",
      "episode reward: -0.7600000000000005\n",
      "episode reward: -0.8700000000000006\n",
      "episode reward: -1.1900000000000008\n",
      "********************************************************\n",
      "steps: 36027\n",
      "episodes: 270\n",
      "mean 100 episode reward: -1.2\n",
      "% time spent exploring: 0.554165875\n",
      "********************************************************\n",
      "episode reward: -2.8299999999999836\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -0.9300000000000006\n",
      "episode reward: -0.6300000000000003\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -0.7000000000000004\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -0.7400000000000004\n",
      "episode reward: -3.089999999999978\n",
      "episode reward: -0.8600000000000005\n",
      "********************************************************\n",
      "steps: 37483\n",
      "episodes: 280\n",
      "mean 100 episode reward: -1.2\n",
      "% time spent exploring: 0.5361478749999999\n",
      "********************************************************\n",
      "episode reward: -0.8200000000000005\n",
      "episode reward: -0.5700000000000003\n",
      "episode reward: -0.9000000000000006\n",
      "episode reward: -0.9500000000000006\n",
      "episode reward: -1.1900000000000008\n",
      "episode reward: -0.6500000000000004\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -0.8100000000000005\n",
      "episode reward: -1.1100000000000008\n",
      "episode reward: -1.2100000000000009\n",
      "********************************************************\n",
      "steps: 38586\n",
      "episodes: 290\n",
      "mean 100 episode reward: -1.2\n",
      "% time spent exploring: 0.5224982499999999\n",
      "********************************************************\n",
      "episode reward: -0.5100000000000002\n",
      "episode reward: -0.6800000000000004\n",
      "episode reward: -0.8200000000000005\n",
      "episode reward: -0.9800000000000006\n",
      "episode reward: -0.7500000000000004\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -1.1300000000000008\n",
      "episode reward: -0.9200000000000006\n",
      "episode reward: -0.9800000000000006\n",
      "episode reward: -0.8300000000000005\n",
      "********************************************************\n",
      "steps: 39622\n",
      "episodes: 300\n",
      "mean 100 episode reward: -1.2\n",
      "% time spent exploring: 0.50967775\n",
      "********************************************************\n",
      "episode reward: -0.6900000000000004\n",
      "episode reward: -1.1000000000000008\n",
      "episode reward: -0.7900000000000005\n",
      "episode reward: -0.9300000000000006\n",
      "episode reward: -1.1100000000000008\n",
      "episode reward: -3.2499999999999747\n",
      "episode reward: -0.9400000000000006\n",
      "episode reward: -0.9300000000000006\n",
      "episode reward: -4.929999999999939\n",
      "episode reward: -1.260000000000001\n",
      "********************************************************\n",
      "steps: 41498\n",
      "episodes: 310\n",
      "mean 100 episode reward: -1.2\n",
      "% time spent exploring: 0.48646225\n",
      "********************************************************\n",
      "episode reward: -0.7900000000000005\n",
      "episode reward: -0.7900000000000005\n",
      "episode reward: -0.9700000000000006\n",
      "episode reward: -1.260000000000001\n",
      "episode reward: -1.460000000000001\n",
      "episode reward: -0.9000000000000006\n",
      "episode reward: -1.280000000000001\n",
      "episode reward: -1.440000000000001\n",
      "episode reward: -0.9700000000000006\n",
      "episode reward: -1.0300000000000007\n",
      "********************************************************\n",
      "steps: 42796\n",
      "episodes: 320\n",
      "mean 100 episode reward: -1.2\n",
      "% time spent exploring: 0.47039949999999997\n",
      "********************************************************\n",
      "episode reward: -1.380000000000001\n",
      "episode reward: -5.119999999999935\n",
      "episode reward: -0.5400000000000003\n",
      "episode reward: -3.839999999999962\n",
      "episode reward: -1.0100000000000007\n",
      "episode reward: -1.5400000000000011\n",
      "episode reward: -0.8000000000000005\n",
      "episode reward: -3.6699999999999657\n",
      "episode reward: -1.440000000000001\n",
      "episode reward: -1.2400000000000009\n",
      "********************************************************\n",
      "steps: 45153\n",
      "episodes: 330\n",
      "mean 100 episode reward: -1.3\n",
      "% time spent exploring: 0.441231625\n",
      "********************************************************\n",
      "episode reward: -1.310000000000001\n",
      "episode reward: -0.6500000000000004\n",
      "episode reward: -0.9700000000000006\n",
      "episode reward: -1.270000000000001\n",
      "episode reward: -0.9900000000000007\n",
      "episode reward: -1.2300000000000009\n",
      "episode reward: -0.9400000000000006\n",
      "episode reward: -1.0800000000000007\n",
      "episode reward: -1.1200000000000008\n",
      "episode reward: -1.0200000000000007\n",
      "********************************************************\n",
      "steps: 46416\n",
      "episodes: 340\n",
      "mean 100 episode reward: -1.2\n",
      "% time spent exploring: 0.4256019999999999\n",
      "********************************************************\n",
      "episode reward: -0.9900000000000007\n",
      "episode reward: -3.8799999999999613\n",
      "episode reward: -1.5300000000000011\n",
      "episode reward: -1.2300000000000009\n",
      "episode reward: -1.1100000000000008\n",
      "episode reward: -1.270000000000001\n",
      "episode reward: -1.0200000000000007\n",
      "episode reward: -0.8800000000000006\n",
      "episode reward: -0.8000000000000005\n",
      "episode reward: -1.6000000000000012\n",
      "********************************************************\n",
      "steps: 48080\n",
      "episodes: 350\n",
      "mean 100 episode reward: -1.3\n",
      "% time spent exploring: 0.40501\n",
      "********************************************************\n",
      "episode reward: -0.7200000000000004\n",
      "episode reward: -1.5600000000000012\n",
      "episode reward: -1.1500000000000008\n",
      "episode reward: -1.9000000000000015\n",
      "episode reward: -4.729999999999944\n",
      "episode reward: -1.0200000000000007\n",
      "episode reward: -5.339999999999931\n",
      "episode reward: -1.2000000000000008\n",
      "episode reward: -1.6000000000000012\n",
      "episode reward: -3.3499999999999726\n",
      "********************************************************\n",
      "steps: 50620\n",
      "episodes: 360\n",
      "mean 100 episode reward: -1.4\n",
      "% time spent exploring: 0.3735775\n",
      "********************************************************\n",
      "episode reward: -4.90999999999994\n",
      "episode reward: -8.389999999999866\n",
      "episode reward: -2.6999999999999864\n",
      "episode reward: -1.8100000000000014\n",
      "episode reward: -5.489999999999927\n",
      "episode reward: -8.349999999999866\n",
      "episode reward: -2.259999999999996\n",
      "episode reward: -2.5999999999999885\n",
      "episode reward: -5.35999999999993\n",
      "episode reward: -4.319999999999952\n",
      "********************************************************\n",
      "steps: 55618\n",
      "episodes: 370\n",
      "mean 100 episode reward: -1.7\n",
      "% time spent exploring: 0.31172725\n",
      "********************************************************\n",
      "episode reward: -3.609999999999967\n",
      "episode reward: -6.629999999999903\n",
      "episode reward: -9.359999999999845\n",
      "episode reward: -4.4499999999999496\n",
      "episode reward: -4.249999999999954\n",
      "episode reward: -3.329999999999973\n",
      "episode reward: -2.6499999999999875\n",
      "episode reward: -1.2100000000000009\n",
      "episode reward: -8.749999999999858\n",
      "episode reward: -9.229999999999848\n",
      "********************************************************\n",
      "steps: 61299\n",
      "episodes: 380\n",
      "mean 100 episode reward: -2.0\n",
      "% time spent exploring: 0.24142487499999998\n",
      "********************************************************\n",
      "episode reward: -3.1999999999999758\n",
      "episode reward: -5.079999999999936\n",
      "episode reward: -7.399999999999887\n",
      "episode reward: -4.229999999999954\n",
      "episode reward: -9.57999999999984\n",
      "episode reward: -9.759999999999836\n",
      "episode reward: -9.60999999999984\n",
      "episode reward: -9.669999999999838\n",
      "episode reward: -5.519999999999927\n",
      "episode reward: -6.829999999999899\n",
      "********************************************************\n",
      "steps: 68665\n",
      "episodes: 390\n",
      "mean 100 episode reward: -2.7\n",
      "% time spent exploring: 0.150270625\n",
      "********************************************************\n",
      "episode reward: -9.56999999999984\n",
      "episode reward: -5.5299999999999265\n",
      "episode reward: -9.399999999999844\n",
      "episode reward: -9.839999999999835\n",
      "episode reward: -9.699999999999838\n",
      "episode reward: -9.739999999999837\n",
      "episode reward: -9.789999999999836\n",
      "episode reward: -9.739999999999837\n",
      "episode reward: -9.789999999999836\n",
      "episode reward: -9.879999999999834\n",
      "********************************************************\n",
      "steps: 78209\n",
      "episodes: 400\n",
      "mean 100 episode reward: -3.5\n",
      "% time spent exploring: 0.03216362500000003\n",
      "********************************************************\n",
      "episode reward: -9.919999999999833\n",
      "episode reward: -9.919999999999833\n",
      "episode reward: -9.959999999999832\n",
      "episode reward: -9.949999999999832\n",
      "episode reward: -9.979999999999832\n",
      "episode reward: -9.949999999999832\n",
      "episode reward: -9.979999999999832\n",
      "episode reward: -9.979999999999832\n"
     ]
    }
   ],
   "source": [
    "guest_hard = run_dqn(env,number_episodes=500,max_episode_length=200,iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bcab6ea1c617506e9b62b87d8cb59b3a1093964830b29dfb83ef363aee7d949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
