{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from gym import spaces\n",
    "from torch.optim import Optimizer\n",
    "import numpy as np\n",
    "# from dqn.model import DQN\n",
    "# from dqn.replay_buffer import ReplayBuffer\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "import random\n",
    "from nle import nethack\n",
    "import minihack\n",
    "from PIL import Image\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    \"\"\"Formats state into form that the NN can accept\"\"\"\n",
    "    glyphs = state[\"glyphs\"]\n",
    "    # Normalize\n",
    "    glyphs = glyphs/glyphs.max()\n",
    "    glyphs = glyphs.reshape((1,1,21,79))\n",
    "    return torch.from_numpy(glyphs).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_screen(state):\n",
    "    \"\"\"Displays the state as the screen in image form using the 'pixel' observation key\"\"\"\n",
    "    screen = Image.fromarray(np.uint8(state['pixel']))\n",
    "    display(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Simple storage for transitions from an environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Initialise a buffer of a given size for storing transitions\n",
    "        :param size: the maximum number of transitions that can be stored\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
    "        :param state: the agent's initial state\n",
    "        :param action: the action taken by the agent\n",
    "        :param reward: the reward the agent received\n",
    "        :param next_state: the subsequent state\n",
    "        :param done: whether the episode terminated\n",
    "        \"\"\"\n",
    "        data = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, indices):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in indices:\n",
    "            data = self._storage[i]\n",
    "            state, action, reward, next_state, done = data\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the buffer.\n",
    "        :param batch_size: the number of transitions to sample\n",
    "        :return: a mini-batch of sampled transitions\n",
    "        \"\"\"\n",
    "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
    "        return self._encode_sample(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of a Deep Q-Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space: spaces.Discrete):\n",
    "        \"\"\"\n",
    "        Initialise the DQN\n",
    "        :param action_space: the action space of the environment\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20,kernel_size=(5, 5))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50,kernel_size=(5, 5))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = nn.Linear(in_features=1600, out_features=500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=action_space.n)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns the values of a forward pass of the network\n",
    "        :param x: The input to feed into the network \n",
    "        \"\"\"\n",
    "        # define first conv layer with max pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # define second conv layer with max pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # Define fully connected layers\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class that brings all the DQN compenents together so that a model can be trained\n",
    "class DQNAgent():\n",
    "    def __init__(self, observation_space, action_space, **kwargs):\n",
    "        global device\n",
    "        self.action_space = action_space\n",
    "        self.replay_buffer = kwargs.get(\"replay_buffer\", None)\n",
    "        self.use_double_dqn = kwargs.get(\"use_double_dqn\", None)\n",
    "        self.gamma = kwargs.get(\"gamma\", 0.99)\n",
    "        self.lr = kwargs.get(\"lr\", None)\n",
    "        self.betas = kwargs.get(\"betas\", (0.9, 0.999))\n",
    "        self.batch_size = kwargs.get(\"batch_size\", None)\n",
    "        # Create the online and target network\n",
    "        self.online_network = DQN(action_space).to(device)\n",
    "        self.target_network = DQN(action_space).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.online_network.parameters(), lr=self.lr, betas=self.betas)\n",
    "   \n",
    "\n",
    "    def optimise_td_loss(self):\n",
    "        \"\"\"\n",
    "        Optimise the TD-error over a single minibatch of transitions\n",
    "        :return: the loss\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.use_double_dqn:\n",
    "                _, max_next_action = self.online_network(next_states).max(1)\n",
    "                max_next_q_values = self.target_network(next_states).gather(1, max_next_action.unsqueeze(1)).squeeze()\n",
    "            else:\n",
    "                next_q_values = self.online_network(next_states)\n",
    "                max_next_q_values, _ = next_q_values.max(1)\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        input_q_values = self.target_network(states)\n",
    "        input_q_values = input_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = F.smooth_l1_loss(input_q_values, target_q_values)\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        del states\n",
    "        del next_states\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Update the target Q-network by copying the weights from the current Q-network\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\"Select action base on network inference\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            observation = observation.type(torch.FloatTensor) \n",
    "        else:\n",
    "            observation = observation.type(torch.cuda.FloatTensor) \n",
    "        state = torch.unsqueeze(observation, 0).to(device)\n",
    "        result = self.online_network.forward(state)\n",
    "        action = torch.argmax(result).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(env, seed, learning_rate, max_episodes, max_episode_length, gamma, verbose=True):\n",
    "\n",
    "    hyper_params = {\n",
    "        'replay-buffer-size': 100000, # replay buffer size\n",
    "        'gamma': 0.99,  # discount factor\n",
    "        'learning-rate': learning_rate, # learning rate for Adam optimizer\n",
    "        'num-steps': int(2e5),  # total number of steps to run the environment for\n",
    "        'batch-size': 256,  # number of transitions to optimize at the same time\n",
    "        'learning-starts': 50000,  # set learning to start after 1000 steps of exploration\n",
    "        'learning-freq': 5,  # number of iterations between every optimization step\n",
    "        'use-double-dqn': True, # use double deep Q-learning\n",
    "        'target-update-freq': 50000, # number of iterations between every target network update\n",
    "        'eps-start': 1.0,  # e-greedy start threshold \n",
    "        'eps-end': 0.08,  # e-greedy end threshold \n",
    "        'eps-fraction': 0.08,  # fraction of num-steps\n",
    "        'print-freq': 10,\n",
    "\n",
    "    }\n",
    "\n",
    "    np.random.seed(42)\n",
    "    env.seed(42)\n",
    "    \n",
    "    # Create DQN agent\n",
    "    replay_buffer = ReplayBuffer(hyper_params['replay-buffer-size'])\n",
    "    agent = DQNAgent(\n",
    "        env.observation_space, \n",
    "        env.action_space,\n",
    "        train=True,\n",
    "        replay_buffer=replay_buffer,\n",
    "        use_double_dqn=hyper_params['use-double-dqn'],\n",
    "        lr=hyper_params['learning-rate'],\n",
    "        batch_size=hyper_params['batch-size'],\n",
    "        gamma=hyper_params['gamma'],\n",
    "    )\n",
    "    \n",
    "    # define variables to track agent metrics\n",
    "    total_reward = 0\n",
    "    scores = []\n",
    "    mean_rewards = []\n",
    "\n",
    "    eps_timesteps = hyper_params[\"eps-fraction\"] * float(hyper_params[\"num-steps\"])\n",
    "    episode_rewards = [0.0]\n",
    "\n",
    "    # Reset gym env before training\n",
    "    state = format_state(env.reset())\n",
    "    eps_timesteps = hyper_params['eps-fraction'] * float(hyper_params['num-steps'])\n",
    "    # Train for set number of steps\n",
    "    for t in range(hyper_params['num-steps']):\n",
    "        # determine exploration probability\n",
    "        fract = min(1.0, float(t) / eps_timesteps)\n",
    "        eps_threshold = hyper_params[\"eps-start\"] + fract * (hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"])\n",
    "        sample = random.random()\n",
    "        # Decide to explore and choose random action or use model to act\n",
    "        if sample < eps_threshold:\n",
    "            action = np.random.choice(agent.action_space.n)\n",
    "        else:\n",
    "            action = agent.act(state)\n",
    "        # Take step in environment\n",
    "        (next_state, reward, done, _) = env.step(action)\n",
    "        next_state = format_state(next_state)\n",
    "        replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            scores.append(total_reward)\n",
    "            print(f\"episode reward: {total_reward}\")\n",
    "            np.random.seed(seed)\n",
    "            env.seed(seed)\n",
    "            state = format_state(env.reset())\n",
    "            total_reward = 0\n",
    "\n",
    "        if (\n",
    "            t > hyper_params[\"learning-starts\"]\n",
    "            and t % hyper_params[\"learning-freq\"] == 0\n",
    "        ):\n",
    "            agent.optimise_td_loss()\n",
    "\n",
    "        if (\n",
    "            t > hyper_params[\"learning-starts\"]\n",
    "            and t % hyper_params[\"target-update-freq\"] == 0\n",
    "        ):\n",
    "            agent.update_target_network()\n",
    "\n",
    "        num_episodes = len(scores)\n",
    "        if done and hyper_params['print-freq'] is not None and len(scores) % hyper_params['print-freq'] == 0:\n",
    "            clear_output(wait=True)\n",
    "            mean_100ep_reward = round(np.mean(scores[-101:-1]), 1)\n",
    "            mean_rewards.append(mean_100ep_reward)\n",
    "            print('********************************************************')\n",
    "            print('steps: {}'.format(t))\n",
    "            print('episodes: {}'.format(num_episodes))\n",
    "            print('mean 100 episode reward: {}'.format(mean_100ep_reward))\n",
    "            print('% time spent exploring: {}'.format(eps_threshold))\n",
    "            print('********************************************************')\n",
    "            #env.render()\n",
    "  \n",
    "        if num_episodes >=max_episodes:\n",
    "            return scores\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(env,number_episodes,max_episode_length,iterations):\n",
    "    \"\"\"Trains DQN model for a number of episodes on a given environment\"\"\"\n",
    "    seeds = np.random.randint(1000, size=iterations)\n",
    "    scores_arr = [] \n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Train the DQN Model \n",
    "        scores = dqn(env=env, \n",
    "                        seed=42, \n",
    "                        learning_rate=1e-3,\n",
    "                        max_episodes=number_episodes, \n",
    "                        max_episode_length=max_episode_length, \n",
    "                        gamma=0.99 ,\n",
    "                        verbose=True)\n",
    "        # Store rewards for this iteration \n",
    "        scores_arr.append(scores)\n",
    "        \n",
    "    return scores_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1264x336>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MOVE_ACTIONS = tuple(nethack.CompassDirection)\n",
    "NAVIGATE_ACTIONS = MOVE_ACTIONS + (\n",
    "    nethack.Command.PICKUP,\n",
    "    nethack.Command.APPLY,\n",
    "    nethack.Command.FIRE,\n",
    "    nethack.Command.RUSH,\n",
    "    nethack.Command.ZAP,\n",
    "    nethack.Command.PUTON,\n",
    "    nethack.Command.READ,\n",
    "    nethack.Command.WEAR,\n",
    "    nethack.Command.QUAFF,\n",
    "    nethack.Command.PRAY,\n",
    "    nethack.Command.OPEN\n",
    "    )\n",
    "env = gym.make(\"MiniHack-Quest-Hard-v0\", observation_keys=[\"glyphs\",\"pixel\",\"message\"], actions=NAVIGATE_ACTIONS)\n",
    "# Reset the environment and display the screen of the starting state \n",
    "display_screen(env.reset())\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "steps: 83478\n",
      "episodes: 150\n",
      "mean 100 episode reward: -7.7\n",
      "% time spent exploring: 0.07999999999999996\n",
      "********************************************************\n",
      "episode reward: -9.929999999999833\n",
      "episode reward: -9.919999999999833\n"
     ]
    }
   ],
   "source": [
    "guest_hard = run_dqn(env,number_episodes=2000,max_episode_length=200,iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.7200000000000004,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4300000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -1.7100000000000013,\n",
       "  -0.4100000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4300000000000002,\n",
       "  -0.7200000000000004,\n",
       "  -0.4000000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4300000000000002,\n",
       "  -0.4300000000000002,\n",
       "  -0.4300000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4300000000000002,\n",
       "  -0.4000000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.4400000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -2.0500000000000003,\n",
       "  -0.4400000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -2.479999999999991,\n",
       "  -0.4300000000000002,\n",
       "  -1.7200000000000013,\n",
       "  -0.4100000000000002,\n",
       "  -0.45000000000000023,\n",
       "  -0.4200000000000002,\n",
       "  -1.7200000000000013,\n",
       "  -0.4200000000000002,\n",
       "  -1.7900000000000014,\n",
       "  -0.4300000000000002,\n",
       "  -1.0300000000000007,\n",
       "  -0.4200000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.45000000000000023,\n",
       "  -0.9200000000000006,\n",
       "  -1.7400000000000013,\n",
       "  -0.6100000000000003,\n",
       "  -0.6000000000000003,\n",
       "  -0.7300000000000004,\n",
       "  -0.4300000000000002,\n",
       "  -0.4400000000000002,\n",
       "  -0.4300000000000002,\n",
       "  -2.1799999999999975,\n",
       "  -0.4300000000000002,\n",
       "  -0.4100000000000002,\n",
       "  -0.47000000000000025,\n",
       "  -0.4200000000000002,\n",
       "  -0.4300000000000002,\n",
       "  -0.4400000000000002,\n",
       "  -0.7700000000000005,\n",
       "  -0.4400000000000002,\n",
       "  -0.4200000000000002,\n",
       "  -0.48000000000000026,\n",
       "  -1.8200000000000014,\n",
       "  -0.4200000000000002,\n",
       "  -0.48000000000000026,\n",
       "  -2.1799999999999975,\n",
       "  -0.8000000000000005,\n",
       "  -2.0899999999999994,\n",
       "  -0.4200000000000002,\n",
       "  -0.4400000000000002,\n",
       "  -1.9300000000000015,\n",
       "  -0.45000000000000023,\n",
       "  -2.349999999999994,\n",
       "  -0.4400000000000002,\n",
       "  -0.4400000000000002,\n",
       "  -1.8800000000000014,\n",
       "  -0.6100000000000003,\n",
       "  -1.8300000000000014,\n",
       "  -0.45000000000000023,\n",
       "  -0.49000000000000027,\n",
       "  -0.4300000000000002,\n",
       "  -5.079999999999936,\n",
       "  -2.769999999999985,\n",
       "  -0.8100000000000005,\n",
       "  -0.45000000000000023,\n",
       "  -0.5000000000000002,\n",
       "  -0.7600000000000005,\n",
       "  -1.9300000000000015,\n",
       "  -0.47000000000000025,\n",
       "  -0.47000000000000025,\n",
       "  -0.9300000000000006,\n",
       "  -1.9100000000000015,\n",
       "  -0.4300000000000002,\n",
       "  -0.8800000000000006,\n",
       "  -0.6500000000000004,\n",
       "  -2.1199999999999988,\n",
       "  -1.1600000000000008,\n",
       "  -0.46000000000000024,\n",
       "  -0.8000000000000005,\n",
       "  -0.8700000000000006,\n",
       "  -0.47000000000000025,\n",
       "  -2.629999999999988,\n",
       "  -2.0000000000000013,\n",
       "  -2.0300000000000007,\n",
       "  -2.149999999999998,\n",
       "  -1.9400000000000015,\n",
       "  -0.6400000000000003,\n",
       "  -0.48000000000000026,\n",
       "  -0.8500000000000005,\n",
       "  -2.020000000000001,\n",
       "  -2.0799999999999996,\n",
       "  -2.489999999999991,\n",
       "  -0.8700000000000006,\n",
       "  -2.759999999999985,\n",
       "  -0.47000000000000025,\n",
       "  -2.1899999999999973,\n",
       "  -2.0500000000000003,\n",
       "  -1.9600000000000015,\n",
       "  -0.8700000000000006,\n",
       "  -1.9800000000000015,\n",
       "  -2.5099999999999905,\n",
       "  -2.5999999999999885,\n",
       "  -3.6799999999999655,\n",
       "  -2.099999999999999,\n",
       "  -0.9300000000000006,\n",
       "  -0.48000000000000026,\n",
       "  -0.9100000000000006,\n",
       "  -2.07,\n",
       "  -1.0600000000000007,\n",
       "  -0.5200000000000002,\n",
       "  -0.5400000000000003,\n",
       "  -2.0300000000000007,\n",
       "  -2.2299999999999964,\n",
       "  -0.5300000000000002,\n",
       "  -2.6899999999999866,\n",
       "  -0.8900000000000006,\n",
       "  -0.7300000000000004,\n",
       "  -2.0799999999999996,\n",
       "  -1.5200000000000011,\n",
       "  -2.7399999999999856,\n",
       "  -2.6499999999999875,\n",
       "  -0.5400000000000003,\n",
       "  -0.8600000000000005,\n",
       "  -2.909999999999982,\n",
       "  -0.9800000000000006,\n",
       "  -3.3399999999999728,\n",
       "  -0.9600000000000006,\n",
       "  -2.289999999999995,\n",
       "  -0.5400000000000003,\n",
       "  -0.8900000000000006,\n",
       "  -1.2300000000000009,\n",
       "  -0.9400000000000006,\n",
       "  -0.5900000000000003,\n",
       "  -0.9000000000000006,\n",
       "  -2.1799999999999975,\n",
       "  -2.7399999999999856,\n",
       "  -2.819999999999984,\n",
       "  -3.569999999999968,\n",
       "  -2.899999999999982,\n",
       "  -0.8800000000000006,\n",
       "  -1.8800000000000014,\n",
       "  -3.00999999999998,\n",
       "  -1.2000000000000008,\n",
       "  -1.0200000000000007,\n",
       "  -2.3599999999999937,\n",
       "  -2.53999999999999,\n",
       "  -0.6200000000000003,\n",
       "  -2.6099999999999883,\n",
       "  -3.4899999999999696,\n",
       "  -1.0100000000000007,\n",
       "  -3.0199999999999796,\n",
       "  -3.8799999999999613,\n",
       "  -2.4699999999999913,\n",
       "  -3.4499999999999704,\n",
       "  -8.989999999999853,\n",
       "  -4.859999999999941,\n",
       "  -1.290000000000001,\n",
       "  -3.899999999999961,\n",
       "  -1.2100000000000009,\n",
       "  -0.6700000000000004,\n",
       "  -2.1799999999999975,\n",
       "  -2.9799999999999804,\n",
       "  -3.289999999999974,\n",
       "  -1.7800000000000014,\n",
       "  -2.909999999999982,\n",
       "  -3.9499999999999598,\n",
       "  -0.6700000000000004,\n",
       "  -4.379999999999951,\n",
       "  -2.859999999999983,\n",
       "  -1.260000000000001,\n",
       "  -4.4499999999999496,\n",
       "  -3.3499999999999726,\n",
       "  -1.2100000000000009,\n",
       "  -0.7000000000000004,\n",
       "  -3.149999999999977,\n",
       "  -3.92999999999996,\n",
       "  -3.45999999999997,\n",
       "  -3.7299999999999645,\n",
       "  -1.370000000000001,\n",
       "  -0.9100000000000006,\n",
       "  -3.089999999999978,\n",
       "  -9.14999999999985,\n",
       "  -4.87999999999994,\n",
       "  -3.989999999999959,\n",
       "  -5.749999999999922,\n",
       "  -4.239999999999954,\n",
       "  -5.179999999999934,\n",
       "  -4.589999999999947,\n",
       "  -3.709999999999965,\n",
       "  -5.139999999999935,\n",
       "  -3.5299999999999687,\n",
       "  -5.589999999999925,\n",
       "  -3.989999999999959,\n",
       "  -5.009999999999938,\n",
       "  -3.7799999999999634,\n",
       "  -4.269999999999953,\n",
       "  -3.619999999999967,\n",
       "  -9.199999999999848,\n",
       "  -2.7399999999999856,\n",
       "  -5.749999999999922,\n",
       "  -9.259999999999847,\n",
       "  -2.8799999999999826,\n",
       "  -4.399999999999951,\n",
       "  -5.269999999999932,\n",
       "  -6.589999999999904,\n",
       "  -8.719999999999859,\n",
       "  -4.979999999999938,\n",
       "  -7.2199999999998905,\n",
       "  -9.489999999999842,\n",
       "  -7.919999999999876,\n",
       "  -5.5299999999999265,\n",
       "  -5.609999999999925,\n",
       "  -7.589999999999883,\n",
       "  -9.639999999999839,\n",
       "  -9.689999999999838,\n",
       "  -9.749999999999837,\n",
       "  -8.389999999999866,\n",
       "  -5.069999999999936,\n",
       "  -9.10999999999985,\n",
       "  -9.809999999999835,\n",
       "  -5.549999999999926,\n",
       "  -9.749999999999837,\n",
       "  -9.13999999999985,\n",
       "  -9.15999999999985,\n",
       "  -9.819999999999835,\n",
       "  -9.749999999999837,\n",
       "  -9.779999999999836,\n",
       "  -9.819999999999835,\n",
       "  -9.789999999999836,\n",
       "  -9.929999999999833,\n",
       "  -2.949999999999981,\n",
       "  -9.899999999999833,\n",
       "  -9.839999999999835,\n",
       "  -9.899999999999833,\n",
       "  -8.059999999999873,\n",
       "  -9.909999999999833,\n",
       "  -9.939999999999833,\n",
       "  -9.959999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.959999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.939999999999833,\n",
       "  -9.979999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.959999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.969999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.969999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.959999999999832,\n",
       "  -9.979999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.989999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.959999999999832,\n",
       "  -9.999999999999831,\n",
       "  -9.999999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.979999999999832,\n",
       "  -9.989999999999831,\n",
       "  -9.999999999999831]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guest_hard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d54dffb68f87f4dcc284e0a5b55b240de8cec84a69957843d5f33711bca4ad99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
